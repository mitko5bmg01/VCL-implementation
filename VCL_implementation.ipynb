{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSX2qaMrp4oP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSCKLNktp5TN"
      },
      "outputs": [],
      "source": [
        "# compute KL(X || Y)\n",
        "# where X and Y are d-dim. vectors with indep. coordinates s.t.\n",
        "# X_i ~ N(mu_x_i, exp(log_sigma_x_i)) and\n",
        "# Y_i ~ N(mu_y_i, exp(log_sigma_y_i))\n",
        "def KL_divergence(mu_x, log_sigma_x, mu_y, log_sigma_y):\n",
        "    kl = log_sigma_y - log_sigma_x + 0.5 * (torch.exp(2 * log_sigma_x) + \\\n",
        "           (mu_x - mu_y)**2) / torch.exp(2 * log_sigma_y) - 0.5\n",
        "\n",
        "    axes_to_reduce = list(range(1, len(mu_x.shape)))\n",
        "    return torch.sum(kl, axes_to_reduce)\n",
        "\n",
        "\n",
        "# compute KL(X || Y)\n",
        "# where X and Y are d-dim. vectors with indep. coordinates s.t.\n",
        "# X_i ~ Lap(mu_x_i, exp(log_b_x_i)) and\n",
        "# Y_i ~ Lap(mu_y_i, exp(log_b_y_i))\n",
        "def KL_divergence(mu_x, log_b_x, mu_y, log_b_y):\n",
        "    kl = (torch.exp(log_b_x - torch.abs(mu_x - mu_y) / torch.exp(log_b_x)) + \\\n",
        "         torch.abs(mu_x - mu_y)) / torch.exp(log_b_y) + log_b_y - log_b_x - 1.0\n",
        "\n",
        "    axes_to_reduce = list(range(1, len(mu_x.shape)))\n",
        "    return torch.sum(kl, axes_to_reduce)\n",
        "\n",
        "# sample from N ~ N(mu, log_sigma)\n",
        "def sample_gaussian(mu, log_sigma):\n",
        "    return mu + torch.exp(log_sigma) * torch.randn(mu.shape).to(mu.device)\n",
        "\n",
        "# sample from L ~ Lap(mu, log_b)\n",
        "def sample_laplace(mu, log_b):\n",
        "    shape = mu.get_shape()\n",
        "    x = torch.log(torch.rand(shape, dtype=tf.float32)) - \\\n",
        "        torch.log(torch.rand(shape, dtype=tf.float32))\n",
        "    return mu + torch.exp(log_b) * x\n",
        "\n",
        "def bernoulli_log_likelihood(x, mu):\n",
        "    log_likelihood = x * torch.log(torch.clamp(mu, 1e-9, 1.0)) \\\n",
        "                      + (1.0 - x) * torch.log(torch.clamp(1.0 - mu, 1e-9, 1.0))\n",
        "\n",
        "    axes_to_reduce = list(range(1, len(x.shape)))\n",
        "    return torch.sum(log_likelihood, axes_to_reduce)\n",
        "\n",
        "def normal_log_likelihood(x, mu, log_sigma):\n",
        "    log_likelihood = -0.5 * np.log(2 * np.pi) - log_sigma \\\n",
        "                      - 0.5 * ((x - mu) / torch.exp(log_sigma)) ** 2\n",
        "\n",
        "    axes_to_reduce = list(range(1, len(x.shape)))\n",
        "    return torch.sum(log_likelihood, axes_to_reduce)\n",
        "\n",
        "def laplace_log_likelihood(x, mu, log_sigma):\n",
        "    log_likelihood = -log_sigma - torch.abs(x - mu) / torch.exp(log_sigma) - np.log(2.0)\n",
        "\n",
        "    axes_to_reduce = list(range(1, len(x.shape)))\n",
        "    return torch.sum(log_likelihood, axes_to_reduce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Iozxftvp7VT"
      },
      "outputs": [],
      "source": [
        "class RandomisedLinearModule(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, activation):\n",
        "        super().__init__()\n",
        "\n",
        "        scale = np.sqrt(6.0/(in_dim + out_dim))\n",
        "\n",
        "        self.mu_W = nn.Parameter(torch.rand(out_dim, in_dim) * 2 * scale - scale)\n",
        "        self.log_sigma_W = nn.Parameter(torch.ones(out_dim, in_dim, dtype=torch.float32) * (-6.0))\n",
        "        self.mu_b = nn.Parameter(torch.zeros(out_dim, dtype=torch.float32))\n",
        "        self.log_sigma_b = nn.Parameter(torch.ones(out_dim, dtype=torch.float32) * (-6.0))\n",
        "\n",
        "        self.mu_W_prior = torch.zeros(out_dim, in_dim, dtype=torch.float32, device=\"cuda:0\")\n",
        "        self.log_sigma_W_prior = torch.zeros(out_dim, in_dim, dtype=torch.float32, device=\"cuda:0\")\n",
        "        self.mu_b_prior = torch.zeros(out_dim, dtype=torch.float32, device=\"cuda:0\")\n",
        "        self.log_sigma_b_prior = torch.zeros(out_dim, dtype=torch.float32, device=\"cuda:0\")\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x, sampling_mode=True):\n",
        "        if sampling_mode:\n",
        "            #W = sample_laplace(self.mu_W, self.log_sigma_W)\n",
        "            #b = sample_laplace(self.mu_b, self.log_sigma_b)\n",
        "            W = sample_gaussian(self.mu_W, self.log_sigma_W)\n",
        "            b = sample_gaussian(self.mu_b, self.log_sigma_b)\n",
        "        else:\n",
        "            W = self.mu_W\n",
        "            b = self.mu_b\n",
        "\n",
        "        return self.activation(F.linear(x, W, b))\n",
        "\n",
        "    def KL_div(self):\n",
        "        return KL_divergence(self.mu_W, self.log_sigma_W, self.mu_W_prior, self.log_sigma_W_prior).sum() + \\\n",
        "               KL_divergence(self.mu_b, self.log_sigma_b, self.mu_b_prior, self.log_sigma_b_prior).sum()\n",
        "\n",
        "    def update_prior(self):\n",
        "        with torch.no_grad():\n",
        "            self.mu_W_prior.copy_(self.mu_W)\n",
        "            self.log_sigma_W_prior.copy_(self.log_sigma_W)\n",
        "            self.mu_b_prior.copy_(self.mu_b)\n",
        "            self.log_sigma_b_prior.copy_(self.log_sigma_b)\n",
        "\n",
        "    def reset_log_sigmas(self):\n",
        "        with torch.no_grad():\n",
        "            self.log_sigma_W.copy_(torch.full_like(self.log_sigma_W, -6.0))\n",
        "            self.log_sigma_b.copy_(torch.full_like(self.log_sigma_b, -6.0))\n",
        "\n",
        "class SharedModule(nn.Module):\n",
        "    def __init__(self, dim_x, dim_h, n_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.layers_list = nn.ModuleList(\n",
        "            [RandomisedLinearModule(dim_h, dim_h, F.relu) for _ in range(n_layers-1)] + \\\n",
        "            [RandomisedLinearModule(dim_h, dim_x, F.sigmoid)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, sampling_mode=True):\n",
        "        for layer in self.layers_list:\n",
        "            x = layer(x, sampling_mode)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def KL_div(self):\n",
        "        kl_div = 0.0\n",
        "        for layer in self.layers_list:\n",
        "            kl_div += layer.KL_div()\n",
        "\n",
        "        return kl_div\n",
        "\n",
        "    def update_prior(self):\n",
        "        for layer in self.layers_list:\n",
        "            layer.update_prior()\n",
        "\n",
        "    def reset_log_sigmas(self):\n",
        "        for layer in self.layers_list:\n",
        "            layer.reset_log_sigmas()\n",
        "\n",
        "\n",
        "class TaskSpecificModule(nn.Module):\n",
        "    def __init__(self, dim_z, dim_h, n_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers_list = nn.ModuleList(\n",
        "            [RandomisedLinearModule(dim_z, dim_h, F.relu)] + \\\n",
        "            [RandomisedLinearModule(dim_h, dim_h, F.relu) for _ in range(n_layers-1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, sampling_mode=True):\n",
        "        for layer in self.layers_list:\n",
        "            x = layer(x, sampling_mode)\n",
        "        return x\n",
        "\n",
        "class GeneratorModule(nn.Module):\n",
        "    def __init__(self, dim_z, dim_x, dim_h, n_tasks, n_layers_shared, n_layers_taskspec):\n",
        "        super().__init__()\n",
        "\n",
        "        self.shared_module = SharedModule(dim_x, dim_h, n_layers_shared)\n",
        "\n",
        "        self.taskspec_modules = nn.ModuleList(\n",
        "            [TaskSpecificModule(dim_z, dim_h, n_layers_taskspec) for _ in range(n_tasks)]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, task_ind, sampling_mode=True):\n",
        "        x = self.taskspec_modules[task_ind](z, sampling_mode)\n",
        "        x = self.shared_module(x, sampling_mode)\n",
        "        return x\n",
        "\n",
        "    def KL_div_shared_prior_post(self):\n",
        "        return self.shared_module.KL_div()\n",
        "\n",
        "    def update_shared_params_prior(self):\n",
        "        self.shared_module.update_prior()\n",
        "\n",
        "    def reset_shared_params_log_sigmas(self):\n",
        "        self.shared_module.reset_log_sigmas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPS-GtuIs-iS"
      },
      "outputs": [],
      "source": [
        "class LinearModule(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        scale = np.sqrt(6.0/(in_dim + out_dim))\n",
        "\n",
        "        self.W = nn.Parameter(torch.rand(out_dim, in_dim, dtype=torch.float32) * 2 * scale - scale)\n",
        "        self.b = nn.Parameter(torch.zeros(out_dim, dtype=torch.float32))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.W, self.b)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim_z, dim_x, dim_h, n_layers_shared, n_layers_taskspec):\n",
        "        super().__init__()\n",
        "        self.n_inner_layers = n_layers_shared + n_layers_taskspec\n",
        "\n",
        "        self.mlp_module = nn.ModuleList(\n",
        "            [LinearModule(dim_x, dim_h)] + \\\n",
        "            [LinearModule(dim_h, dim_h) for _ in range(self.n_inner_layers - 2)] + \\\n",
        "            [LinearModule(dim_h, 2*dim_z)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.mlp_module):\n",
        "            x = layer(x)\n",
        "            if i < self.n_inner_layers - 1:\n",
        "                x = F.relu(x)\n",
        "\n",
        "        mu, log_sigma = x[:, :x.shape[1] // 2], x[:, (x.shape[1] // 2):]\n",
        "\n",
        "        return mu, log_sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSXKsMettAmi"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'dim_z' : 50,\n",
        "    'dim_h' : 500,\n",
        "    'dim_x' : 28 ** 2,\n",
        "    'n_layers_shared': 2,\n",
        "    'n_layers_taskspec': 2,\n",
        "    'n_tasks': 10,\n",
        "    'batch_size': 50,\n",
        "    'n_epochs': 200,\n",
        "    'lr': 1e-4,\n",
        "    'sampling_mode': False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJur6_n2tFnr"
      },
      "outputs": [],
      "source": [
        "# alter these when changing the dataset\n",
        "\n",
        "task_labels = list(range(hyperparams['n_tasks']))\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k4GUPrttI3Y"
      },
      "outputs": [],
      "source": [
        "task_traindata_dict = {label: None for label in task_labels}\n",
        "task_testdata_dict = {label: None for label in task_labels}\n",
        "\n",
        "for label in task_labels:\n",
        "    train_mask = [i for i in range(len(trainset)) if trainset.targets[i] == label]\n",
        "    test_mask = [i for i in range(len(testset)) if testset.targets[i] == label]\n",
        "\n",
        "    task_traindata_dict[label] = DataLoader(\n",
        "        torch.utils.data.Subset(trainset, train_mask),\n",
        "        batch_size=hyperparams['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    task_testdata_dict[label] = DataLoader(\n",
        "        torch.utils.data.Subset(testset, test_mask),\n",
        "        batch_size=hyperparams['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = GeneratorModule(\n",
        "    hyperparams['dim_z'],\n",
        "    hyperparams['dim_x'],\n",
        "    hyperparams['dim_h'],\n",
        "    hyperparams['n_tasks'],\n",
        "    hyperparams['n_layers_shared'],\n",
        "    hyperparams['n_layers_taskspec']\n",
        ").to(device)\n",
        "\n",
        "encoder = Encoder(\n",
        "    hyperparams['dim_z'],\n",
        "    hyperparams['dim_x'],\n",
        "    hyperparams['dim_h'],\n",
        "    hyperparams['n_layers_shared'],\n",
        "    hyperparams['n_layers_taskspec']\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpP8NpB3tiPg"
      },
      "outputs": [],
      "source": [
        "def compute_loss(x_train, enc, gen, task, total_data, K=10):\n",
        "    # K - number of samples used for Monte Carlo approximation of the expectation\n",
        "    z_post_mu, z_post_log_sigma = enc(x_train)\n",
        "\n",
        "    # compute KL(q_phi(z|x) || p(z))   knowing z has indep N(0,1) coordinates\n",
        "    kl_z_prior_post = KL_divergence(z_post_mu, z_post_log_sigma,\n",
        "                                       torch.zeros(z_post_mu.shape).to(device), torch.zeros(z_post_mu.shape).to(device))\n",
        "\n",
        "    # compute KL divergence between prior and posterior of shared params\n",
        "    # KL(q_t(theta) || q_(t-1)(theta))\n",
        "    kl_shared_param = gen.KL_div_shared_prior_post()\n",
        "\n",
        "    # estimate E_q_phi_z [log(p_theta(x|z))]\n",
        "    # we model x ~ Ber(gen(z))\n",
        "    post_x_log_likelihood = 0.0\n",
        "    axes_to_reduce = list(range(1, len(x_train.shape)))\n",
        "\n",
        "    for _ in range(K):\n",
        "        #z = sample_laplace(z_post_mu, z_post_log_sigma)\n",
        "        z = sample_gaussian(z_post_mu, z_post_log_sigma)\n",
        "        x_mu = gen(z, task)\n",
        "        post_x_log_likelihood += bernoulli_log_likelihood(x_train, x_mu) / K\n",
        "\n",
        "    loss = kl_z_prior_post.mean() - post_x_log_likelihood.mean() + kl_shared_param / total_data\n",
        "\n",
        "    return loss, kl_z_prior_post.mean().item(), post_x_log_likelihood.mean().item(), kl_shared_param.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q1TvlPDuhRI"
      },
      "outputs": [],
      "source": [
        "def eval_test_ll_on_task(task_testdata_dict, task, enc, gen, sampling_mode=True, K=100):\n",
        "    enc.eval(); gen.eval()\n",
        "\n",
        "    total_batches = len(task_testdata_dict[task])\n",
        "\n",
        "    test_ll_mean = 0.0\n",
        "    test_ll_std = 0.0\n",
        "\n",
        "    for data_batch, _ in task_testdata_dict[task]:\n",
        "        data_batch = data_batch.to(device)\n",
        "\n",
        "        xs_stacked = torch.tile(data_batch.view(-1, 28**2), (K, 1))\n",
        "        z_post_mu, z_post_log_sigma = enc(xs_stacked)\n",
        "        #z = sample_laplace(z_post_mu, z_post_log_sigma)\n",
        "        z = sample_gaussian(z_post_mu, z_post_log_sigma)\n",
        "\n",
        "        #z_prior_log_likelihood = laplace_log_likelihood(z, torch.zeros(z.shape).to(device), torch.zeros(z.shape).to(device))\n",
        "        z_prior_log_likelihood = normal_log_likelihood(z, torch.zeros(z.shape).to(device), torch.zeros(z.shape).to(device))\n",
        "        #z_post_log_likelihood = laplace_log_likelihood(z, z_post_mu, z_post_log_sigma)\n",
        "        z_post_log_likelihood = normal_log_likelihood(z, z_post_mu, z_post_log_sigma)\n",
        "        kl_z_prior_post = z_post_log_likelihood - z_prior_log_likelihood\n",
        "\n",
        "        xs_stacked_mu = gen(z, task, sampling_mode)\n",
        "        x_post_log_likelihood = bernoulli_log_likelihood(xs_stacked, xs_stacked_mu)\n",
        "\n",
        "        bound = (x_post_log_likelihood - kl_z_prior_post).reshape(K, data_batch.shape[0])\n",
        "        bound_max = torch.max(bound, dim=0).values\n",
        "        bound -= bound_max\n",
        "\n",
        "        log_norm_bound = torch.log(torch.clamp(torch.mean(torch.exp(bound), 0), 1e-9, np.inf))\n",
        "        test_ll = log_norm_bound + bound_max\n",
        "        test_ll_mean += torch.mean(test_ll) / total_batches\n",
        "        test_ll_std += torch.sqrt(torch.var(test_ll, correction=0) / test_ll.shape[0]) / total_batches\n",
        "\n",
        "    return test_ll_mean, test_ll_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yElB9gfHuHF2"
      },
      "outputs": [],
      "source": [
        "def train(task_traindata_dict, task_testdata_dict, enc, gen, hyperparams, device):\n",
        "    adam = Adam(list(enc.parameters()) + list(gen.parameters()), lr=hyperparams['lr'])\n",
        "    accuracies = []\n",
        "\n",
        "    for task, data in enumerate(task_traindata_dict.values()):\n",
        "        enc.train(); gen.train()\n",
        "        tmp_accuracies = []\n",
        "        print(\"Data for task\", task, \"arrives.\")\n",
        "\n",
        "        for epoch in range(hyperparams['n_epochs']):\n",
        "            kl1s = []; expecs = []; kl2s = []\n",
        "\n",
        "            total_data = len(data.sampler)\n",
        "\n",
        "            for data_batch, _ in data:\n",
        "                data_batch = data_batch.to(device)\n",
        "                adam.zero_grad()\n",
        "                loss, kl1, expec, kl2 = compute_loss(\n",
        "                    data_batch.view(-1, 28**2),\n",
        "                    enc, gen, task, total_data\n",
        "                )\n",
        "                kl1s.append(kl1); kl2s.append(kl2); expecs.append(expec)\n",
        "                loss.backward()\n",
        "                adam.step()\n",
        "\n",
        "            print('epoch:', epoch, 'with stats', sum(kl1s)/len(kl1s), sum(expecs)/len(expecs), sum(kl2s)/len(kl2s))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enc.eval(); gen.eval()\n",
        "\n",
        "            for prev_task in range(task + 1):\n",
        "                #z = torch.log(torch.rand(hyperparams['dim_z'], dtype=tf.float32)) - \\\n",
        "                #    torch.log(torch.rand(hyperparams['dim_z'], dtype=tf.float32))\n",
        "                z = torch.randn(hyperparams['dim_z']).to(device)\n",
        "                x = generator(z, prev_task, sampling_mode=hyperparams['sampling_mode']).reshape(28, 28)\n",
        "                plt.imshow(x.cpu().detach().numpy(), cmap='gray', vmin=0.0, vmax=1.0)\n",
        "                plt.show()\n",
        "\n",
        "                test_ll_mean, test_ll_std = eval_test_ll_on_task(\n",
        "                    task_testdata_dict, prev_task, enc, gen,\n",
        "                    sampling_mode=hyperparams['sampling_mode']\n",
        "                )\n",
        "\n",
        "                print(\"On task:\", list(task_testdata_dict.keys())[prev_task],\n",
        "                      \"was achieved test_ll_mean:\", test_ll_mean,\n",
        "                      \"with test_ll_std:\", test_ll_std\n",
        "                )\n",
        "\n",
        "                tmp_accuracies.append((test_ll_mean, test_ll_std))\n",
        "\n",
        "        gen.update_shared_params_prior()\n",
        "        gen.reset_shared_params_log_sigmas()\n",
        "\n",
        "        accuracies.append(tmp_accuracies)\n",
        "\n",
        "    return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBi5RlTLuekU"
      },
      "outputs": [],
      "source": [
        "accuracies = train(task_traindata_dict, task_testdata_dict, encoder, generator, hyperparams, device)\n",
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkwVMN-xutDg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}